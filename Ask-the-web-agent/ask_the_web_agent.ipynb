{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7fe8ac",
   "metadata": {},
   "source": [
    "# Project 3: **Ask‚Äëthe‚ÄëWeb Agent**\n",
    "\n",
    "Welcome to Project‚ÄØ3! In this project, you will learn how to use tool‚Äëcalling LLMs, extend them with custom tools, and build a simplified *Perplexity‚Äëstyle* agent that answers questions by searching the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4311a6",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Understand why tool calling is useful and how LLMs can invoke external tools.\n",
    "* Implement a minimal loop that parses the LLM's output and executes a Python function.\n",
    "* See how *function schemas* (docstrings and type hints) let us scale to many tools.\n",
    "* Use **LangChain** to get function‚Äëcalling capability for free (ReAct reasoning, memory, multi‚Äëstep planning).\n",
    "* Combine LLM with a web‚Äësearch tool to build a simple ask‚Äëthe‚Äëweb agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f16864",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "1. Environment setup\n",
    "2. Write simple tools and connect them to an LLM\n",
    "3. Standardize tool calling by writing `to_schema`\n",
    "4. Use LangChain to augment an LLM with your tools\n",
    "5. Build a Perplexity‚Äëstyle web‚Äësearch agent\n",
    "6. (Optional) A minimal backend and frontend UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae51d0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# 1- Environment setup\n",
    "\n",
    "## 1.1- Conda environment\n",
    "\n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook and run:\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yml && conda activate web_agent\n",
    "\n",
    "# Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=web_agent --display-name \"web_agent\"\n",
    "```\n",
    "Once this is done, you can select ‚Äúweb_agent‚Äù from the Kernel ‚Üí Change Kernel menu in Jupyter or VS Code.\n",
    "\n",
    "\n",
    "> Behind the scenes:\n",
    "> * Conda reads `environment.yml`, resolves the pinned dependencies, creates an isolated environment named `web_agent`, and activates it.\n",
    "> * `ollama pull` downloads the model so you can run it locally without API calls.\n",
    "\n",
    "\n",
    "## 1.2 Ollama setup\n",
    "\n",
    "In this project, we start with `gemma3-1B` because it is lightweight and runs on most machines. You can try other smaller or larger LLMs such as `mistral:7b`, `phi3:mini`, or `llama3.2:1b` to compare performance. Explore available models here: https://ollama.com/library\n",
    "\n",
    "```bash\n",
    "ollama pull gemma3:1b\n",
    "```\n",
    "\n",
    "`ollama pull` downloads the model so you can run it locally without API calls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27158f",
   "metadata": {},
   "source": [
    "## 2- Tool¬†Calling\n",
    "\n",
    "LLMs are strong at answering questions, but they cannot directly access external data such as live web results, APIs, or computations. In real applications, agents rarely rely only on their internal knowledge. They need to query APIs, retrieve data, or perform calculations to stay accurate and useful. Tool calling bridges this gap by allowing the LLM to request actions from the outside world.\n",
    "\n",
    "\n",
    "We describe each tool‚Äôs interface in the model‚Äôs prompt, defining what it does and what arguments it expects. When the model decides that a tool is needed, it emits a structured output like: `TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Francisco\"}}`. Your code will detect this output, execute the corresponding function, and feed the result back to the LLM so the conversation continues.\n",
    "\n",
    "In this section, you will implement a simple `get_current_weather` function and teach the `gemma3` model how to use it when required in four steps:\n",
    "1. Implement the tool\n",
    "2. Create the instructions for the LLM\n",
    "3. Call the LLM with the prompt\n",
    "4. Parse the LLM output and call the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a536f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is 18¬∞C and foggy in San Francisco.\n",
      "It is 23¬∞C and sunny in San Diego.\n",
      "It is 53¬∞F and rainy in Seattle.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Implement the tool\n",
    "# ---------------------------------------------------------\n",
    "# Your goal: give the model a way to access weather information.\n",
    "# You can either:\n",
    "#   (a) Call a real weather API (for example, OpenWeatherMap), or\n",
    "#   (b) Create a dummy function that returns a fixed response (e.g., \"It is 23¬∞C and sunny in San Francisco.\")\n",
    "#\n",
    "# Requirements:\n",
    "#   ‚Ä¢ The function should be named `get_current_weather`\n",
    "#   ‚Ä¢ It should take two arguments:\n",
    "#         - city: str\n",
    "#         - unit: str = \"celsius\"\n",
    "#   ‚Ä¢ Return a short, human-readable sentence describing the weather.\n",
    "#\n",
    "# Example expected behavior:\n",
    "#   get_current_weather(\"San Francisco\") ‚Üí \"It is 23¬∞C and sunny in San Francisco.\"\n",
    "#\n",
    "\n",
    "def get_current_weather(city: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"Get the current weather for a given city.\"\"\"\n",
    "    # Simple dummy implementation - returns realistic weather data\n",
    "    weather_data = {\n",
    "        \"San Francisco\": (\"18¬∞C\", \"foggy\"),\n",
    "        \"San Diego\": (\"23¬∞C\", \"sunny\"),\n",
    "        \"Seattle\": (\"12¬∞C\", \"rainy\"),\n",
    "        \"New York\": (\"15¬∞C\", \"cloudy\"),\n",
    "        \"London\": (\"10¬∞C\", \"rainy\"),\n",
    "        \"Paris\": (\"16¬∞C\", \"partly cloudy\"),\n",
    "        \"Tokyo\": (\"20¬∞C\", \"clear\"),\n",
    "    }\n",
    "    \n",
    "    temp, condition = weather_data.get(city, (\"20¬∞C\", \"pleasant\"))\n",
    "    \n",
    "    if unit.lower() == \"fahrenheit\":\n",
    "        # Convert to Fahrenheit for display\n",
    "        celsius = int(temp.replace(\"¬∞C\", \"\"))\n",
    "        fahrenheit = int(celsius * 9/5 + 32)\n",
    "        temp = f\"{fahrenheit}¬∞F\"\n",
    "    \n",
    "    return f\"It is {temp} and {condition} in {city}.\"\n",
    "\n",
    "# Test the function\n",
    "print(get_current_weather(\"San Francisco\"))\n",
    "print(get_current_weather(\"San Diego\"))\n",
    "print(get_current_weather(\"Seattle\", \"fahrenheit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a43c3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM PROMPT:\n",
      "============================================================\n",
      "You are a helpful assistant with access to tools. When you need information that requires a tool, respond with:\n",
      "TOOL_CALL: {\"name\": \"tool_name\", \"args\": {\"arg1\": \"value1\", \"arg2\": \"value2\"}}\n",
      "\n",
      "Available tools:\n",
      "- get_current_weather: Get the current weather for a city\n",
      "  Arguments:\n",
      "    - city (str): The name of the city\n",
      "    - unit (str, optional): Temperature unit, either \"celsius\" or \"fahrenheit\". Defaults to \"celsius\"\n",
      "\n",
      "Example:\n",
      "User: \"What's the weather in Tokyo?\"\n",
      "Assistant: TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"Tokyo\"}}\n",
      "\n",
      "Only use TOOL_CALL when you need to retrieve information. Otherwise, respond naturally.\n",
      "\n",
      "============================================================\n",
      "USER QUESTION:\n",
      "============================================================\n",
      "What is the weather in San Diego today?\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Create the prompt for the LLM to call tools\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Build the system and user prompts that instruct the model when and how\n",
    "#   to use your tool (`get_current_weather`).\n",
    "#\n",
    "# What to include:\n",
    "#   ‚Ä¢ A SYSTEM_PROMPT that tells the model about the tool use and describe the tool\n",
    "#   ‚Ä¢ A USER_QUESTION with a user query that should trigger the tool.\n",
    "#       Example: \"What is the weather in San Diego today?\"\n",
    "\n",
    "# Try experimenting with different system and user prompts\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant with access to tools. When you need information that requires a tool, respond with:\n",
    "TOOL_CALL: {\"name\": \"tool_name\", \"args\": {\"arg1\": \"value1\", \"arg2\": \"value2\"}}\n",
    "\n",
    "Available tools:\n",
    "- get_current_weather: Get the current weather for a city\n",
    "  Arguments:\n",
    "    - city (str): The name of the city\n",
    "    - unit (str, optional): Temperature unit, either \"celsius\" or \"fahrenheit\". Defaults to \"celsius\"\n",
    "  \n",
    "Example:\n",
    "User: \"What's the weather in Tokyo?\"\n",
    "Assistant: TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"Tokyo\"}}\n",
    "\n",
    "Only use TOOL_CALL when you need to retrieve information. Otherwise, respond naturally.\"\"\"\n",
    "\n",
    "USER_QUESTION = \"What is the weather in San Diego today?\"\n",
    "\n",
    "# Display the prompts\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM PROMPT:\")\n",
    "print(\"=\" * 60)\n",
    "print(SYSTEM_PROMPT)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"USER QUESTION:\")\n",
    "print(\"=\" * 60)\n",
    "print(USER_QUESTION)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebb062",
   "metadata": {},
   "source": [
    "Now that you have defined a tool and shown the model how to use it, the next step is to call the LLM using your prompt.\n",
    "\n",
    "Start the **Ollama** server in a terminal with `ollama serve`. This launches a local API endpoint that listens for LLM requests. Once the server is running, return to the notebook and in the next cell send a query to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027cb75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LLM Response:\n",
      "============================================================\n",
      " TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\"}}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 3: Call the LLM with your prompt\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Send SYSTEM_PROMPT + USER_QUESTION to the model.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Use the Ollama client to create a chat completion. \n",
    "#       - You may find some examples here: https://platform.openai.com/docs/api-reference/chat/create\n",
    "#       - If you are unsure, search the web for \"client.chat.completions.create\"\n",
    "#   2. Print the raw response.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should return something like:\n",
    "#   TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\"}}\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create the chat completion request\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistral\",  # Using Mistral as we pulled it earlier\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_QUESTION}\n",
    "    ],\n",
    "    temperature=0  # Low temperature for more deterministic outputs\n",
    ")\n",
    "\n",
    "# Extract and print the model's response\n",
    "llm_output = response.choices[0].message.content\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LLM Response:\")\n",
    "print(\"=\" * 60)\n",
    "print(llm_output)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94aeb4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool `get_current_weather` with args {'city': 'San Diego'}\n",
      "Result: It is 23¬∞C and sunny in San Diego.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 4: Parse the LLM output and call the tool\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Detect when the model requests a tool, extract its name and arguments,\n",
    "#   and execute the corresponding function.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Search for the text pattern \"TOOL_CALL:{...}\" in the model output.\n",
    "#   2. Parse the JSON inside it to get the tool name and args.\n",
    "#   3. Call the matching function (e.g., get_current_weather).\n",
    "#\n",
    "# Expected:\n",
    "#   You should see a line like:\n",
    "#       Calling tool `get_current_weather` with args {'city': 'San Diego'}\n",
    "#       Result: It is 23¬∞C and sunny in San Diego.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import re, json\n",
    "\n",
    "# Search for TOOL_CALL pattern in the LLM output\n",
    "# Look for the JSON object after \"TOOL_CALL:\"\n",
    "if \"TOOL_CALL:\" in llm_output:\n",
    "    # Extract everything after \"TOOL_CALL:\" and find the JSON\n",
    "    tool_call_start = llm_output.find(\"TOOL_CALL:\") + len(\"TOOL_CALL:\")\n",
    "    tool_call_text = llm_output[tool_call_start:].strip()\n",
    "    \n",
    "    # Find the JSON object (between first { and matching })\n",
    "    brace_count = 0\n",
    "    json_end = 0\n",
    "    for i, char in enumerate(tool_call_text):\n",
    "        if char == '{':\n",
    "            brace_count += 1\n",
    "        elif char == '}':\n",
    "            brace_count -= 1\n",
    "            if brace_count == 0:\n",
    "                json_end = i + 1\n",
    "                break\n",
    "    \n",
    "    tool_call_json = tool_call_text[:json_end]\n",
    "    tool_call_data = json.loads(tool_call_json)\n",
    "    \n",
    "    tool_name = tool_call_data[\"name\"]\n",
    "    tool_args = tool_call_data[\"args\"]\n",
    "    \n",
    "    print(f\"Calling tool `{tool_name}` with args {tool_args}\")\n",
    "    \n",
    "    # Execute the tool (mapping tool names to functions)\n",
    "    if tool_name == \"get_current_weather\":\n",
    "        result = get_current_weather(**tool_args)\n",
    "        print(f\"Result: {result}\")\n",
    "    else:\n",
    "        print(f\"Unknown tool: {tool_name}\")\n",
    "else:\n",
    "    print(\"No tool call detected in the LLM output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be26661",
   "metadata": {},
   "source": [
    "# 3- Standadize tool calling\n",
    "\n",
    "So far, we handled tool calling manually by writing one regex and one hard-coded function. This approach does not scale if we want to add more tools. Adding more tools would mean more `if/else` blocks and manual edits to the `TOOL_SPEC` prompt.\n",
    "\n",
    "To make the system flexible, we can standardize tool definitions by automatically reading each function‚Äôs signature, converting it to a JSON schema, and passing that schema to the LLM. This way, the LLM can dynamically understand which tools exist and how to call them without requiring manual updates to prompts or conditional logic.\n",
    "\n",
    "Next, you will implement a small helper that extracts metadata from functions and builds a schema for each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce911b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Get the current weather for a given city.',\n",
      " 'name': 'get_current_weather',\n",
      " 'parameters': {'city': {'required': True, 'type': 'str'},\n",
      "                'unit': {'default': 'celsius',\n",
      "                         'required': False,\n",
      "                         'type': 'str'}}}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Generate a JSON schema for a tool automatically\n",
    "# ---------------------------------------------------------\n",
    "#\n",
    "# Steps:\n",
    "#   1. Use `inspect.signature` to get function parameters.\n",
    "#   2. For each argument, record its name, type, and description.\n",
    "#   3. Build a schema containing:\n",
    "#   4. Test your helper on `get_current_weather` and print the result.\n",
    "#\n",
    "# Expected:\n",
    "#   A dictionary describing the tool (its name, args, and types).\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pprint import pprint\n",
    "import inspect\n",
    "\n",
    "\n",
    "def to_schema(fn):\n",
    "    \"\"\"Convert a function to a JSON schema for tool calling.\"\"\"\n",
    "    # Get function signature\n",
    "    sig = inspect.signature(fn)\n",
    "    \n",
    "    # Build the schema dictionary\n",
    "    schema = {\n",
    "        \"name\": fn.__name__,\n",
    "        \"description\": fn.__doc__.strip() if fn.__doc__ else \"\",\n",
    "        \"parameters\": {}\n",
    "    }\n",
    "    \n",
    "    # Process each parameter\n",
    "    for param_name, param in sig.parameters.items():\n",
    "        param_info = {}\n",
    "        \n",
    "        # Get type annotation\n",
    "        if param.annotation != inspect.Parameter.empty:\n",
    "            param_info[\"type\"] = param.annotation.__name__\n",
    "        else:\n",
    "            param_info[\"type\"] = \"any\"\n",
    "        \n",
    "        # Check if parameter has a default value\n",
    "        if param.default != inspect.Parameter.empty:\n",
    "            param_info[\"default\"] = param.default\n",
    "            param_info[\"required\"] = False\n",
    "        else:\n",
    "            param_info[\"required\"] = True\n",
    "        \n",
    "        schema[\"parameters\"][param_name] = param_info\n",
    "    \n",
    "    return schema\n",
    "\n",
    "tool_schema = to_schema(get_current_weather)\n",
    "pprint(tool_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1163f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LLM Response with Schema:\n",
      "============================================================\n",
      " TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\"}}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Provide the tool schema to the model\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Give the model a \"menu\" of available tools so it can choose\n",
    "#   which one to call based on the user's question.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Add an extra system message (e.g., name=\"tool_spec\")\n",
    "#      containing the JSON schema(s) of your tools.\n",
    "#   2. Include SYSTEM_PROMPT and the user question as before.\n",
    "#   3. Send the messages to the model (e.g., gemma3:1b).\n",
    "#   4. Print the raw model output to see if it picks the right tool.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should produce a structured TOOL_CALL indicating\n",
    "#   which tool to use and with what arguments.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Generate tool schemas automatically\n",
    "tools = [get_current_weather]  # List of available tools\n",
    "tool_schemas = [to_schema(tool) for tool in tools]\n",
    "\n",
    "# Create a formatted tool specification message\n",
    "tool_spec_message = \"Available tools:\\n\" + json.dumps(tool_schemas, indent=2)\n",
    "\n",
    "# Send the request with tool schemas included\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistral\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"system\", \"content\": f\"Tool specifications:\\n{tool_spec_message}\"},\n",
    "        {\"role\": \"user\", \"content\": USER_QUESTION}\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Extract and print the response\n",
    "schema_llm_output = response.choices[0].message.content\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LLM Response with Schema:\")\n",
    "print(\"=\" * 60)\n",
    "print(schema_llm_output)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ec86e",
   "metadata": {},
   "source": [
    "## 4-‚ÄØLangChain for Tool Calling\n",
    "So far, you built a simple tool-calling pipeline manually. While this helps you understand the logic, it does not scale well when working with multiple tools, complex parsing, or multi-step reasoning.\n",
    "\n",
    "LangChain simplifies this process. You only need to declare your tools, and its *Agent* abstraction handles when to call a tool, how to use it, and how to continue reasoning afterward.\n",
    "\n",
    "In this section, you will use the **ReAct** Agent (Reasoning + Acting). It alternates between reasoning steps and tool use, producing clearer and more reliable results. We will explore reasoning-focused models in more depth next week.\n",
    "\n",
    "The following links might be helpful:\n",
    "- https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html\n",
    "- https://python.langchain.com/docs/integrations/tools/\n",
    "- https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.LLM.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c609d97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool created successfully!\n",
      "Tool name: get_weather\n",
      "Tool description: Get the current weather for a given city.\n",
      "\n",
      "    Args:\n",
      "        city: The name of the city to get weather for\n",
      "        unit: Temperature unit, either 'celsius' or 'fahrenheit' (default: 'celsius')\n",
      "\n",
      "    Returns:\n",
      "        A string describing the current weather conditions\n",
      "\n",
      "Testing tool:\n",
      "It is 12¬∞C and rainy in Seattle.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Define tools for LangChain\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Convert your weather function into a LangChain-compatible tool.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Import `tool` from `langchain.tools`.\n",
    "#   2. Keep your existing `get_current_weather` helper as before.\n",
    "#   3. Create a new function (e.g., get_weather) that calls it.\n",
    "#   4. Add the `@tool` decorator so LangChain can register it automatically.\n",
    "#\n",
    "# Notes:\n",
    "#   ‚Ä¢ The decorator converts your Python function into a standardized tool object.\n",
    "#   ‚Ä¢ Start with keeping the logic simple and offline-friendly.\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"Get the current weather for a given city.\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city to get weather for\n",
    "        unit: Temperature unit, either 'celsius' or 'fahrenheit' (default: 'celsius')\n",
    "    \n",
    "    Returns:\n",
    "        A string describing the current weather conditions\n",
    "    \"\"\"\n",
    "    return get_current_weather(city, unit)\n",
    "\n",
    "# Test the LangChain tool\n",
    "print(\"Tool created successfully!\")\n",
    "print(f\"Tool name: {get_weather.name}\")\n",
    "print(f\"Tool description: {get_weather.description}\")\n",
    "print(f\"\\nTesting tool:\")\n",
    "print(get_weather.invoke({\"city\": \"Seattle\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9552348d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing the agent:\n",
      "============================================================\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m To find out if I need an umbrella in Seattle today, I should check the weather conditions there. The tool provided is `weather_check(city: str)`.\n",
      "\n",
      "Action: weather_check\n",
      "Action Input: Seattle\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIt is 12¬∞C and rainy in Seattle.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Based on the current weather conditions in Seattle, it seems like you might need an umbrella today as it's raining there.\n",
      "Final Answer: Yes, you might need an umbrella in Seattle today.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Final Answer:\n",
      "============================================================\n",
      "Yes, you might need an umbrella in Seattle today.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Initialize the LangChain Agent\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Connect your tool to a local LLM using LangChain's ReAct-style agent.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Import the required classes:\n",
    "#        - ChatOllama (for local model access)\n",
    "#        - initialize_agent, Tool, AgentType\n",
    "#   2. Create an LLM instance (e.g., model=\"gemma3:1b\", temperature=0).\n",
    "#   3. Add your tool(s) to a list\n",
    "#   4. Initialize the agent using initialize_agent\n",
    "#   5. Test the agent with a natural question (e.g., \"Do I need an umbrella in Seattle today?\").\n",
    "#\n",
    "# Expected:\n",
    "#   The model should reason through the question, call your tool,\n",
    "#   and produce a final answer in plain language.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# Create the LLM instance\n",
    "llm = ChatOllama(model=\"mistral\", temperature=0)\n",
    "\n",
    "# Create a simplified tool with single input\n",
    "@tool\n",
    "def weather_check(city: str) -> str:\n",
    "    \"\"\"Get the current weather for a given city. Use this to check weather conditions.\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city to check weather for\n",
    "    \n",
    "    Returns:\n",
    "        A string describing the current weather conditions\n",
    "    \"\"\"\n",
    "    return get_current_weather(city)\n",
    "\n",
    "# Create the tools list\n",
    "tools = [weather_check]\n",
    "\n",
    "# Initialize the ReAct agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Test the agent with a question\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing the agent:\")\n",
    "print(\"=\" * 60)\n",
    "response = agent.invoke({\"input\": \"Do I need an umbrella in Seattle today?\"})\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Final Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9e8fb",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "The console log displays the **Thought‚ÄØ‚Üí‚ÄØAction‚ÄØ‚Üí‚ÄØObservation‚ÄØ‚Üí‚ÄØ‚Ä¶** loop until the agent produces its final answer. Because `verbose=True`, LangChain prints each intermediate reasoning step.\n",
    "\n",
    "If you want to add more tools, simply append them to the tools list. LangChain will handle argument validation, schema generation, and tool-calling logic automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5- Perplexity‚ÄëStyle Web Search\n",
    "Agents become much more powerful when they can look up real information on the web instead of relying only on their internal knowledge.\n",
    "\n",
    "In this section, you will combine everything you have learned to build a simple Ask-the-Web Agent. You will integrate a web search tool (DuckDuckGo) and make it available to the agent using the same tool-calling approach as before.\n",
    "\n",
    "This will let the model retrieve fresh results, reason over them, and generate an informed answer‚Äîsimilar to how Perplexity works.\n",
    "\n",
    "You may find some examples from the following links:\n",
    "- https://pypi.org/project/duckduckgo-search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web search tool created successfully!\n",
      "Tool name: web_search\n",
      "Tool description: Search the web for current information using DuckDuckGo.\n",
      "\n",
      "    Args:\n",
      "        query: The search query string\n",
      "\n",
      "    Returns:\n",
      "        A formatted string with search results including titles and URLs\n",
      "\n",
      "Testing with a sample query...\n",
      "1. Welcome to Python.org\n",
      "   URL: \n",
      "   The official home of the PythonProgrammingLanguageThe official home of the PythonProgrammingLanguageThe official home of the PythonProgrammingLanguageThe official home of the PythonProgrammingLanguage\n",
      "\n",
      "2. Python (programming language)\n",
      "   URL: https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "   Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language. Python 3.0, released in 2008, was a major revision and not completely backward-compatible with earlier versions. Beginning with Python 3.5, capabilities and keywords for typing were added to the language, allowing optional static typing. As of 2025, the Python Software Foundation supports Python 3.10, 3.11, 3.12, 3.13, and 3.14, following the projects annual release cycle and five-year support policy. Earlier versions in the 3.x series have reached end-of-life and no longer receive security updates. Python has gained widespread use in the machine learning community. It is widely taught as an introductory programming language. Since 2003, Python has consistently ranked in the top ten of the most popular programming languages in the TIOBE Programming Community Index, which ranks based on searches in 24 platforms.\n",
      "\n",
      "3. Welcome to Python.org\n",
      "   URL: https://www.python.org/\n",
      "   Python is a versatile and easy-to-learn language that lets you work quickly and integrate systems more effectively. Learn more about Python, download the latest version, access documentation, find jobs, events, success stories and more.\n",
      "\n",
      "4. Introduction to Python - W3SchoolsUsage example\n",
      "   URL: https://www.w3schools.com/python/python_intro.asp\n",
      "   Learn what Python is, what it can do, and why it is a popular programming language. Find out how Python syntax, interpreter, and platforms make it easy and versatile for web development, software development, mathematics, and more. See more on w3schools\n",
      "\n",
      "5. Learn Python - Free Interactive Python Tutorial\n",
      "   URL: https://www.learnpython.org/\n",
      "   Welcome to the LearnPython.org interactive Python tutorial. Whether you are an experienced programmer or not, this website is intended for everyone who wishes to learn the Pythonprogramminglanguage.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Add a web search tool\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Create a tool that lets the agent search the web and return results.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Use DuckDuckGo for quick, open web searches.\n",
    "#   2. Write a helper function (e.g., search_web) that:\n",
    "#        ‚Ä¢ Takes a query string\n",
    "#        ‚Ä¢ Uses DDGS to fetch top results (titles + URLs)\n",
    "#        ‚Ä¢ Returns them as a formatted string\n",
    "#   3. Wrap it with the @tool decorator to make it available to LangChain.\n",
    "\n",
    "\n",
    "from ddgs import DDGS\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for current information using DuckDuckGo.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query string\n",
    "    \n",
    "    Returns:\n",
    "        A formatted string with search results including titles and URLs\n",
    "    \"\"\"\n",
    "    ddgs = DDGS()\n",
    "    results = ddgs.text(query, max_results=5)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No results found.\"\n",
    "    \n",
    "    formatted_results = []\n",
    "    for i, result in enumerate(results, 1):\n",
    "        title = result.get('title', 'No title')\n",
    "        url = result.get('href', 'No URL')\n",
    "        snippet = result.get('body', 'No description')\n",
    "        formatted_results.append(f\"{i}. {title}\\n   URL: {url}\\n   {snippet}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_results)\n",
    "\n",
    "# Test the web search tool\n",
    "print(\"Web search tool created successfully!\")\n",
    "print(f\"Tool name: {web_search.name}\")\n",
    "print(f\"Tool description: {web_search.description}\")\n",
    "print(\"\\nTesting with a sample query...\")\n",
    "test_result = web_search.invoke({\"query\": \"Python programming language\"})\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web-search agent initialized successfully!\n",
      "Agent has 1 tool(s) available: ['web_search']\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Initialize the web-search agent\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Connect your `web_search` tool to a language model\n",
    "#   so the agent can search and reason over real data.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Import `initialize_agent` and `AgentType`.\n",
    "#   2. Create an LLM (e.g., ChatOllama).\n",
    "#   3. Add your `web_search` tool to the tools list.\n",
    "#   4. Initialize the agent using: initialize_agent\n",
    "#   5. Keep `verbose=True` to observe reasoning steps.\n",
    "#\n",
    "# Expected:\n",
    "#   The agent should be ready to accept user queries\n",
    "#   and use your web search tool when needed.\n",
    "# ---------------------------------------------------------\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Create the LLM instance using Mistral\n",
    "llm = ChatOllama(model=\"mistral\", temperature=0)\n",
    "\n",
    "# Create the tools list with our web search tool\n",
    "web_tools = [web_search]\n",
    "\n",
    "# Initialize the web-search agent\n",
    "web_agent = initialize_agent(\n",
    "    tools=web_tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"Web-search agent initialized successfully!\")\n",
    "print(f\"Agent has {len(web_tools)} tool(s) available: {[tool.name for tool in web_tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let‚Äôs see the agent's output in action with a real example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Question: What are the latest developments in artificial intelligence in December 2025?\n",
      "============================================================\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m To find the latest developments in artificial intelligence as of December 2025, I will use the web_search tool.\n",
      "\n",
      "Action: web_search\n",
      "Action Input: \"Latest developments in artificial intelligence December 2025\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m1. Wall Street Sees AI Bubble Coming and Is Betting on... - Bloomberg\n",
      "   URL: https://www.bloomberg.com/news/articles/2025-12-14/wall-street-sees-an-ai-bubble-forming-and-is-gaming-what-pops-it\n",
      "   December 14, 2025 at 5:00 PM GMT+3. Save. Translate. Takeaways by Bloomberg AI.It‚Äôs been three years since OpenAI set off euphoria over artificialintelligence with the release of ChatGPT. And while the money is still pouring in, so are the doubts about whether the good times can last.\n",
      "\n",
      "2. AI Model Releases Nov/Dec2025: Grok 4.1, Gemini 3, Claude...\n",
      "   URL: https://vertu.com/lifestyle/the-ai-model-race-reaches-singularity-speed/\n",
      "   From November 17 through December 11, 2025, the artificialintelligence industry witnessed an unprecedented concentration of frontier model releases that fundamentally reshaped the competitive landscape.\n",
      "\n",
      "3. New AI tool identifies not just genetic mutations, but the diseases they...\n",
      "   URL: https://medicalxpress.com/news/2025-12-ai-tool-genetic-mutations-diseases.html\n",
      "   Scientists at the Icahn School of Medicine at Mount Sinai have developed a novel artificialintelligence tool that not only identifies disease-causing genetic mutations but also predicts the type of disease those mutations may trigger.\n",
      "\n",
      "4. GPT-5.2 Still Counts Two R's In Strawberry - Dataconomy\n",
      "   URL: https://dataconomy.com/2025/12/15/gpt-5-2-still-counts-two-rs-in-strawberry/\n",
      "   ChatGPT, powered by OpenAI's GPT-5.2 model released in December2025, incorrectly identifies two r's in the word strawberry, which contains three, because its.\n",
      "\n",
      "5. White House instructs agencies to stop using ‚Äòbiased‚Äô AI - Nextgov/FCW\n",
      "   URL: https://www.nextgov.com/artificial-intelligence/2025/12/white-house-instructs-agencies-stop-using-biased-ai/410135/\n",
      "   | December 12, 2025. The Office of Management and Budget clarified the steps agencies will have to take to ensure their contracted large language models do not produce ‚Äúwoke‚Äù outputs. White House.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: In December 2025, there were several significant developments in artificial intelligence. One of the major events was the release of multiple advanced AI models such as Grok 4.1, Gemini 3, and Claude. Another notable development was a new AI tool that can not only identify disease-causing genetic mutations but also predict the type of disease those mutations may trigger. Additionally, there were concerns about potential biases in AI systems, leading to the White House instructing agencies to stop using 'biased' AI. Furthermore, there were doubts about the sustainability of the AI market due to the euphoria caused by ChatGPT and OpenAI's GPT-5.2 model.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "============================================================\n",
      "Final Answer:\n",
      "============================================================\n",
      "In December 2025, there were several significant developments in artificial intelligence. One of the major events was the release of multiple advanced AI models such as Grok 4.1, Gemini 3, and Claude. Another notable development was a new AI tool that can not only identify disease-causing genetic mutations but also predict the type of disease those mutations may trigger. Additionally, there were concerns about potential biases in AI systems, leading to the White House instructing agencies to stop using 'biased' AI. Furthermore, there were doubts about the sustainability of the AI market due to the euphoria caused by ChatGPT and OpenAI's GPT-5.2 model.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 3: Test your Ask-the-Web agent\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Verify that the agent can search the web and return\n",
    "#   a summarized answer based on real results.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Ask a natural question that requires live information,\n",
    "#      for example: \"What are the current events in San Francisco this week?\"\n",
    "#   2. Call agent.\n",
    "#\n",
    "# Expected:\n",
    "#   The agent should call `web_search`, retrieve results,\n",
    "#   and generate a short summary response.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Test with a question that requires current web information\n",
    "question = \"What are the latest developments in artificial intelligence in December 2025?\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {question}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Invoke the web agent\n",
    "response = web_agent.invoke({\"input\": question})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Final Answer:\")\n",
    "print(\"=\" * 60)\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6- A minimal UI\n",
    "This project includes a simple **React** front end that sends the user‚Äôs question to a FastAPI back end and streams the agent‚Äôs response in real time. To run the UI:\n",
    "\n",
    "1- Open a terminal and start the Ollama server: `ollama serve`.\n",
    "\n",
    "2- In a second terminal, navigate to the frontend folder and install dependencies:`npm install`.\n",
    "\n",
    "3- In the same terminal, navigate to the backend folder and start the FastAPI back‚Äëend: `uvicorn app:app --reload --port 8000`\n",
    "\n",
    "4- Open a third terminal, navigate to the frontend folder, and start the React dev server: `npm run dev`\n",
    "\n",
    "5- Visit `http://localhost:5173/` in your browser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "* You have built a **web‚Äëenabled agent**: tool calling ‚Üí JSON schema ‚Üí LangChain ReAct ‚Üí web search ‚Üí simple UI.\n",
    "* Try adding more tools, such as news or finance APIs.\n",
    "* Experiment with multiple tools, different models, and measure accuracy vs. hallucination.\n",
    "\n",
    "\n",
    "üëè **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
